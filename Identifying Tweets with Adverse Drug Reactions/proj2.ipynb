{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import operator\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from senti_classifier import senti_classifier\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False)\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"train.txt\", 'r',  encoding='utf-8')\n",
    "tweets = []\n",
    "pos_tweets = []\n",
    "neg_tweets = []\n",
    "ID = []\n",
    "Class = []\n",
    "\n",
    "for line in f:\n",
    "    word = line.split()\n",
    "    ID.append(word[0])\n",
    "    Class.append(word[1])\n",
    "    tweets.append(\" \".join(word[2:]))\n",
    "    if word[1] == \"N\": \n",
    "        pos_tweets.append((\" \".join(word[2:]), word[1]))\n",
    "    else:\n",
    "        neg_tweets.append((\" \".join(word[2:]), word[1]))\n",
    "                                        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many tweets in train set:  3166\n",
      "How many 'N':  2793 instances and 88.219% of the training data\n",
      "How many 'Y':  373 instances and 11.781% of the training data\n"
     ]
    }
   ],
   "source": [
    "print(\"How many tweets in train set: \", len(tweets))\n",
    "print(\"How many 'N': \", len(pos_tweets), \"instances and\", '{:.3%}'.format(len(pos_tweets)/len(tweets)), \"of the training data\")\n",
    "print(\"How many 'Y': \", len(neg_tweets), \"instances and\", '{:.3%}'.format(len(neg_tweets)/len(tweets)), \"of the training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"dev.txt\", 'r',  encoding='utf-8')\n",
    "\n",
    "dev_tweets = []\n",
    "dev_pos_tweets = []\n",
    "dev_neg_tweets = []\n",
    "\n",
    "for line in f:\n",
    "    word = line.split()\n",
    "    dev_tweets.append((\" \".join(word[2:]), word[1]))\n",
    "    if word[1] == \"N\": \n",
    "        dev_pos_tweets.append((\" \".join(word[2:]), word[1]))\n",
    "    else:\n",
    "        dev_neg_tweets.append((\" \".join(word[2:]), word[1]))\n",
    "                                        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many tweets in dev set:  1076\n",
      "How many 'N':  962 instances and 89.405% of the dev data\n",
      "How many 'Y':  114 instances and 10.595% of the dev data\n"
     ]
    }
   ],
   "source": [
    "print(\"How many tweets in dev set: \", len(dev_tweets))\n",
    "print(\"How many 'N': \", len(dev_pos_tweets), \"instances and\", '{:.3%}'.format(len(dev_pos_tweets)/len(dev_tweets)), \"of the dev data\")\n",
    "print(\"How many 'Y': \", len(dev_neg_tweets), \"instances and\", '{:.3%}'.format(len(dev_neg_tweets)/len(dev_tweets)), \"of the dev data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many tweets in train + dev set:  4242\n",
      "How many 'N':  3755 instances and 88.520% of the train + dev data\n",
      "How many 'Y':  487 instances and 11.480% of the train + dev data\n"
     ]
    }
   ],
   "source": [
    "total = len(dev_tweets) + len(tweets)\n",
    "total_pos = len(dev_pos_tweets) + len(pos_tweets)\n",
    "total_neg = len(dev_neg_tweets) + len(neg_tweets)\n",
    "print(\"How many tweets in train + dev set: \", total)\n",
    "print(\"How many 'N': \", total_pos, \"instances and\", '{:.3%}'.format(total_pos/total), \"of the train + dev data\")\n",
    "print(\"How many 'Y': \", total_neg, \"instances and\", '{:.3%}'.format(total_neg/total), \"of the train + dev data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tweets(f):\n",
    "    import pandas as pd\n",
    "    tweets=[]\n",
    "    ID=[]\n",
    "    label=[]\n",
    "    for line in f:\n",
    "        tweets.append(line.split('\\t')[2].strip())    \n",
    "        ID.append(line.split('\\t')[0].strip())    \n",
    "        label.append(line.split('\\t')[1].strip())\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['ID'] = ID\n",
    "    df['Class'] = label\n",
    "    df['Tweet'] = tweets\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"train.txt\", 'r',  encoding='utf-8')\n",
    "f2 = open(\"dev.txt\", 'r',  encoding='utf-8')\n",
    "f3 = open(\"test.txt\", 'r',  encoding='utf-8')\n",
    "\n",
    "train = parse_tweets(f1)\n",
    "dev = parse_tweets(f2)\n",
    "test = parse_tweets(f3)\n",
    "\n",
    "train_dev = train.append(dev, ignore_index=True)\n",
    "train_dev_test = train_dev.append(test, ignore_index=True)\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>326376825590779905</td>\n",
       "      <td>N</td>\n",
       "      <td>Do U know what Medications are R for bipolar d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>326398829849092097</td>\n",
       "      <td>Y</td>\n",
       "      <td>I think my tablets have made me gain weight. A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>326406322323066883</td>\n",
       "      <td>Y</td>\n",
       "      <td>Thought of work is overwhelming me so much I f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>326407491460141056</td>\n",
       "      <td>N</td>\n",
       "      <td>@awakenings_ ziprasidone and olanzapine I.m. B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>326453069795688449</td>\n",
       "      <td>N</td>\n",
       "      <td>#كيف_تتخلص_من_الاكتئاب جرب Venlafaxine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID Class                                              Tweet\n",
       "0  326376825590779905     N  Do U know what Medications are R for bipolar d...\n",
       "1  326398829849092097     Y  I think my tablets have made me gain weight. A...\n",
       "2  326406322323066883     Y  Thought of work is overwhelming me so much I f...\n",
       "3  326407491460141056     N  @awakenings_ ziprasidone and olanzapine I.m. B...\n",
       "4  326453069795688449     N             #كيف_تتخلص_من_الاكتئاب جرب Venlafaxine"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will tweets' lengths give us some clues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_length(df):\n",
    "    a = df[df['Class'] == 'N']\n",
    "    b = df[df['Class'] == 'Y']\n",
    "    \n",
    "    return (a['Tweet'].str.len().mean(), b['Tweet'].str.len().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96.59362692445399, 107.50134048257372)\n",
      "(97.10498960498961, 105.99122807017544)\n"
     ]
    }
   ],
   "source": [
    "print(tweets_length(train))\n",
    "print(tweets_length(dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about punctuations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punctuation(df):\n",
    "    a = df[df['Class'] == 'N']\n",
    "    b = df[df['Class'] == 'Y']\n",
    "    return (a['Tweet'].str.count(r'\\W').mean(), b['Tweet'].str.count(r'\\W').mean())\n",
    "\n",
    "def count_digits(df):\n",
    "    a = df[df['Class'] == 'N']\n",
    "    b = df[df['Class'] == 'Y']\n",
    "    return (a['Tweet'].str.count(r'\\d').mean(), b['Tweet'].str.count(r'\\d').mean())\n",
    "\n",
    "def count_Caps(df):\n",
    "    a = df[df['Class'] == 'N']\n",
    "    b = df[df['Class'] == 'Y']\n",
    "    return (a['Tweet'].str.count(r'[A-Z]').mean(), b['Tweet'].str.count(r'[A-Z]').mean())\n",
    "\n",
    "def count_elongated_words(df):\n",
    "    a = df[df['Class'] == 'N']\n",
    "    b = df[df['Class'] == 'Y']\n",
    "    return (a['Tweet'].str.count(r'(.)\\1{3}').mean(), b['Tweet'].str.count(r'(.)\\1{3}').mean())\n",
    "\n",
    "# def count_quantity(df):\n",
    "#     a = df[df['Class'] == 'N']\n",
    "#     b = df[df['Class'] == 'Y']\n",
    "#     return (a['Tweet'].str.count(r'\\d[a-zA-Z]{1,3}\\s').mean(), b['Tweet'].str.count(r'\\d[a-zA-Z]{1,3}\\s').mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19.82205513784461, 22.171581769436997)\n",
      "(19.805613305613306, 21.649122807017545)\n",
      "\n",
      "\n",
      "(0.9477264590046545, 0.7479892761394102)\n",
      "(1.0322245322245323, 0.8157894736842105)\n",
      "\n",
      "\n",
      "(5.426781238811314, 4.458445040214477)\n",
      "(5.618503118503119, 5.43859649122807)\n",
      "\n",
      "\n",
      "(0.058360186179735055, 0.05093833780160858)\n",
      "(0.05405405405405406, 0.03508771929824561)\n"
     ]
    }
   ],
   "source": [
    "print(count_punctuation(train))\n",
    "print(count_punctuation(dev))\n",
    "print('\\n')\n",
    "print(count_digits(train))\n",
    "print(count_digits(dev))\n",
    "print('\\n')\n",
    "print(count_Caps(train))\n",
    "print(count_Caps(dev))\n",
    "print('\\n')\n",
    "print(count_elongated_words(train))\n",
    "print(count_elongated_words(dev))\n",
    "# print('\\n')\n",
    "# print(count_quantity(train))\n",
    "# print(count_quantity(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tweet_level_features(df):\n",
    "    a = pd.DataFrame()\n",
    "    a['tweet_length'] = df['Tweet'].str.len()\n",
    "    a['count_digits'] = df['Tweet'].str.count(r'\\d')\n",
    "    a['count_punctuations'] = df['Tweet'].str.count(r'\\W')\n",
    "    a['count_CAPITALs'] = df['Tweet'].str.count(r'[A-Z]')\n",
    "    a['count_elongated_words'] = df['Tweet'].str.count(r'(\\w)\\1{3}')\n",
    "    return a\n",
    "\n",
    "a = add_tweet_level_features(train_dev_test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_new = a.iloc[:3166].copy()\n",
    "# dev_new = all_vectorized.iloc[3166:4242].copy()\n",
    "# test_new = all_vectorized.iloc[4242:].copy()\n",
    "# train94 = add_features(train94, a.iloc[:3166].copy())\n",
    "dev94 = add_features(dev94, a.iloc[3166:4242].copy())\n",
    "test94 = add_features(test94, a.iloc[4242:].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's clean the tweets a little bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(df):\n",
    "    list_of_tokens = [tknzr.tokenize(x) for x in df['Tweet']]\n",
    "    list_of_tokens = [cleaning(x) for x in list_of_tokens]\n",
    "    list_of_tokens = pd.DataFrame([\" \".join(x) for x in list_of_tokens], columns=[\"Tweet\"])\n",
    "    return list_of_tokens\n",
    "    \n",
    "def cleaning(tokens):\n",
    "    a = []\n",
    "    for x in tokens:\n",
    "        if re.match(r'^\\W+$', x):\n",
    "            continue\n",
    "        if re.match('@\\w+', x):\n",
    "            continue\n",
    "        if re.match('#\\w+', x):\n",
    "            a.append(x.replace(\"#\", \"\"))\n",
    "            continue\n",
    "        if re.match('.*\\d.*', x):\n",
    "            continue   \n",
    "        if re.match(r'\\W+', x):\n",
    "            continue\n",
    "        a.append(wordnet_lemmatizer.lemmatize(x))     \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('adr_sample.txt', 'r', encoding = 'utf-8')\n",
    "# adr_sample = []\n",
    "# for line in f:\n",
    "#     adr_sample.append(line.strip())\n",
    "# f.close()\n",
    "# adr_sample = pd.DataFrame(adr_sample, columns=['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = tokenizer(train)\n",
    "dev_tweets = tokenizer(dev)\n",
    "test_tweets = tokenizer(test)\n",
    "train_dev_test_tweets = tokenizer(train_dev_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = nltk.FreqDist([item for sublist in [tknzr.tokenize(x) for x in list(adr_sample['Tweet'])] for item in sublist])\n",
    "# sorted(dic.items(), key=operator.itemgetter(1), reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add bigram and trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x747 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17059 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = CountVectorizer(min_df=10, ngram_range=(2,3), analyzer='word').fit(train_dev_test_tweets['Tweet'])\n",
    "all_vectorized = vect.transform(train_dev_test_tweets['Tweet'])\n",
    "all_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectorized = pd.SparseDataFrame(all_vectorized, columns=list(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_vectorized.sort_index(axis=1, inplace=True)\n",
    "all_vectorized.fillna(0, inplace=True)\n",
    "all_vectorized = all_vectorized.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df, df2):\n",
    "    df2.reset_index(drop = True, inplace = True)\n",
    "    labels = df[df.columns[-1]]\n",
    "    df.drop(df.columns[-1], axis=1, inplace=True)\n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    df = pd.concat([df, labels], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = all_vectorized.iloc[:3166].copy()\n",
    "train94 = add_features(train94, train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_new = all_vectorized.iloc[3166:4242].copy()\n",
    "dev94 = add_features(dev94, dev_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new = all_vectorized.iloc[4242:].copy()\n",
    "test94 = add_features(test94, test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = test_all[test_all.columns[list(range(0,2598))]].copy()\n",
    "# a = pd.concat([a, test94], axis=1)\n",
    "# a.drop(a.columns[-1], axis=1, inplace=True)\n",
    "# a = pd.concat([a, test_all.iloc[:, 2598:]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.read_csv('train_all.csv')\n",
    "dev_all = pd.read_csv('dev_all.csv')\n",
    "test_all = pd.read_csv('test_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all.to_csv('test_all.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all.drop(test_all.columns[2598:2691], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# X_train = train_all.iloc[:,:-1].copy()\n",
    "# y_train = train_all.iloc[:,-1].copy()\n",
    "# X_test = dev_all.iloc[:,:-1].copy()\n",
    "# y_test = dev_all.iloc[:,-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clf = Pipeline([\n",
    "#   ('feature_selection', SelectFromModel(LinearSVC(C=0.01, penalty=\"l1\", dual=False))),\n",
    "#   ('classification', LinearSVC())\n",
    "# ])\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# roc_auc_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate polarity score and subjectivity score for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "polarity = []\n",
    "subjectivity = []\n",
    "compound_score = []\n",
    "\n",
    "for item in train:\n",
    "    x = Textcleaner(item)\n",
    "    polarity.append(TextBlob(x).sentiment[0])\n",
    "    subjectivity.append(TextBlob(x).sentiment[1])\n",
    "    compound_score.append(analyzer.polarity_scores(item)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv['polarity'] = polarity\n",
    "train_csv['subjectivity'] = subjectivity\n",
    "train_csv['compound_score'] = compound_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side effect lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('se.txt','r')\n",
    "def parse_se(f):\n",
    "    se = []\n",
    "    for line in f:\n",
    "        se.append(line.strip())\n",
    "    return se\n",
    "side = parse_se(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [tknzr.tokenize(x) for x in train]\n",
    "dev = [tknzr.tokenize(x) for x in dev]\n",
    "test = [tknzr.tokenize(x) for x in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\" \".join(x) for x in train]\n",
    "dev = [\" \".join(x) for x in dev]\n",
    "test = [\" \".join(x) for x in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_effect(tweets):\n",
    "    adr=[]\n",
    "    for x in tweets:\n",
    "        count=0\n",
    "        for y in side:\n",
    "            if y in x:\n",
    "                count=1\n",
    "                adr.append(1)\n",
    "                break\n",
    "        if count == 0:\n",
    "            adr.append(0)\n",
    "    return adr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentiWordNet Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Textcleaner(tweets):\n",
    "    return [re.sub(r'\\W+', ' ', x).strip().lower() for x in tweets] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Textcleaner(train)\n",
    "dev = Textcleaner(dev)\n",
    "test = Textcleaner(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentiWordNet(doc):\n",
    "    sentences = nltk.sent_tokenize(doc)\n",
    "    stokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    taggedlist=[]\n",
    "    for stoken in stokens:        \n",
    "         taggedlist.append(nltk.pos_tag(stoken))\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "    score_list=[]\n",
    "    for idx,taggedsent in enumerate(taggedlist):\n",
    "        score_list.append([])\n",
    "        for idx2,t in enumerate(taggedsent):\n",
    "            newtag=''\n",
    "            lemmatized=wnl.lemmatize(t[0])\n",
    "            if t[1].startswith('NN'):\n",
    "                newtag='n'\n",
    "            elif t[1].startswith('JJ'):\n",
    "                newtag='a'\n",
    "            elif t[1].startswith('V'):\n",
    "                newtag='v'\n",
    "            elif t[1].startswith('R'):\n",
    "                newtag='r'\n",
    "            else:\n",
    "                newtag=''       \n",
    "            if(newtag!=''):    \n",
    "                synsets = list(swn.senti_synsets(lemmatized, newtag))\n",
    "                #Getting average of all possible sentiments, as you requested        \n",
    "                score=0\n",
    "                if(len(synsets)>0):\n",
    "                    for syn in synsets:\n",
    "                        score+=syn.pos_score()-syn.neg_score()\n",
    "                    score_list[idx].append(score/len(synsets))\n",
    "\n",
    "    \n",
    "    sentence_sentiment=[]\n",
    "\n",
    "    for score_sent in score_list:\n",
    "        try:\n",
    "            sentence_sentiment.append(sum([word_score for word_score in score_sent])/len(score_sent))\n",
    "        except ZeroDivisionError:\n",
    "            sentence_sentiment.append(0)\n",
    "    \n",
    "    return sentence_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "for x in train:\n",
    "    a.append(score_sentiWordNet(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dev:\n",
    "    b.append(score_sentiWordNet(x)[0])\n",
    "for x in test:\n",
    "    c.append(score_sentiWordNet(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords=[]\n",
    "for x in train+dev+test:\n",
    "    allwords.extend(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = nltk.FreqDist(allwords)\n",
    "word_features = sorted(wordlist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "word_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features_bernoulli(document):\n",
    "#     document_words = set(document)\n",
    "#     features = {}\n",
    "#     for word in word_features:\n",
    "#         features['contains({})'.format(word[0])] = (word[0] in document_words)\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set = nltk.classify.apply_features(extract_features, tweets_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = nltk.classify.apply_features(extract_features, test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifierNB = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# classifierNB.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = classifierNB.classify_many([x[0] for x in test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"The accuracy score of NB Classifier is:\", len([x for index, x in enumerate(prediction) if x == test_words[index][1]])/len(prediction))\n",
    "# [test_tweets[index][0] for index, x in enumerate(prediction) if x == 'Y' and test_words[index][1] == 'N']\n",
    "# [test_tweets[index][0] for index, x in enumerate(prediction) if x == 'N' and test_words[index][1] == 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train.iloc[:,:-1]\n",
    "# y_train = np.ravel(train.iloc[:,-1:])\n",
    "# X_test = dev.iloc[:,:-1]\n",
    "# y_test = np.ravel(dev.iloc[:,-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total.drop(['id', 'person', 'but', 'am', 'been', 'is', 'it', 'next', \n",
    "#             'bananas', 'can', 'this', 'pic', 'rt', 'same', 'though'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# X = total.iloc[:,:-1]\n",
    "# y = np.ravel(total.iloc[:,-1:])\n",
    "# total.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = MultinomialNB()\n",
    "# clf.fit(X_train, y_train)\n",
    "# predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_scores = cross_val_score(clf, X, y, cv=10)\n",
    "# cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('test.csv')\n",
    "# test = test.iloc[:,:-1]\n",
    "\n",
    "# test_prediction = knn.predict(test)\n",
    "# test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# senti = []\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "# for sentence in new_sample:\n",
    "#     vs = analyzer.polarity_scores(\" \".join(sentence[1]))\n",
    "#     senti.append((\" \".join(sentence[1]), vs))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
